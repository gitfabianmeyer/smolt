{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76213385-11a1-44a8-b79f-f5bd1aeb2387",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "#### Note: Might use many resources, as I calc everything new for each cell / task to ensure partial usability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c65f9fd-f088-4869-b243-b5bcba6e4b66",
   "metadata": {},
   "source": [
    "### 3.1.a.a What is the probability to really see a name if C says so?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef0d2cc-38ad-451c-bc74-88e84ac4281f",
   "metadata": {},
   "source": [
    "1.)in Newspaper: $P(N=Name|C=Yes) = \\frac{P(C=yes|N=Name) * P(N=Name}{P(C=yes)} = \\frac{0.9 * 0.05}{0.9*0.05 + 0.2*0.95} = 0.192$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848b5c2d-e620-4f84-a27e-46d62fbfbb12",
   "metadata": {},
   "source": [
    "2)in S.T: $P(N=Name|C=Yes) = \\frac{P(C=yes|N=Name) * P(N=Name}{P(C=yes)} = \\frac{0.9 * 0.01}{0.9*0.01 + 0.2*0.99} = 0.044$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ec28e3-7473-4d83-9cac-b99ee4061ac8",
   "metadata": {},
   "source": [
    "### 3.1.a.b) How low must the false positive rate P(C=yes|N=not-name) get so that this probability goes up to 50% for both kinds of text?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414fdd90-a0c6-4574-bd44-300fa8ce0835",
   "metadata": {},
   "source": [
    "Replace the false positivity rate (0.2) by x and the result by 0.5 and solve both equations of a) for x:\n",
    "$$\\frac{0.9 * 0.05}{0.9*0.05 + x*0.95} = 0.5, x = 0,047$$ \n",
    "$$\\frac{0.9 * 0.01}{0.9*0.01 + x*0.99} = 0.5, x=0.009$$  \n",
    "The false positivity rates have to be much lower in both cases (0.047 and 0.009) to achieve the desired probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeda02c5-ac36-42d7-ba35-f96948f05920",
   "metadata": {},
   "source": [
    "### 3.1.b are X and Y as defined in the following table independently distributed?  \n",
    " \n",
    "X and Y are not independently distributed:\n",
    "$$P(X_0)=0.4, P(X_1)=0.6, P(Y_a)=0.5, P(Y_b)=0.5$$  \n",
    "$$P(X_0) * P(Y_a) = 0.4*0.5=0.2!=0.3 =P(X_0, Y_a)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6e1a77-82a8-4de4-b532-9d8acd10cd80",
   "metadata": {},
   "source": [
    "### 3.1.c Compute log entropies for:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a23d53e-df13-47d2-a6b1-c1fdb221231b",
   "metadata": {},
   "source": [
    "$H(X)=- \\sum_{x \\in X}{p(x) - log_2(p(x))}$  \n",
    "a)  \n",
    "$$H(X) = -0.4 * log(0.4) - 0.6 * log(0.6) = 0.971$$\n",
    "$$H(Y) = -0.5*log(0.5) - 0.5 * log(0.5) = 1.000$$  \n",
    "b)  \n",
    "$$H(X, Y) = -0.3*log(0.3) - 0.1*log(0.1)- 0.2*log(0.2)-0.4*log(0.4) = 1.846$$  \n",
    "$$H(X|Y) = H(X,Y) - H(Y) = 1.846 - 1.000 = 0.846$$\n",
    "$$H(Y|X) = H(X,Y) - H(X) = 1.846 - 0.971 = 0.875 $$  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07755350-1942-412b-9fdd-4a867cef1faf",
   "metadata": {},
   "source": [
    "c)  \n",
    "$$D(X||Y) = \\sum_{x\\in X}{p(x) * log (\\frac{p(x)}{q(x)} )} = 0.4 * log(\\frac{0.4}{0.5}) + 0.6 * log(\\frac{0.6}{0.5}) = 0.029$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864230ac-89e6-4b5d-ad07-4a69bc3d6bd7",
   "metadata": {},
   "source": [
    "# Part2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b782f73c-9519-4001-a494-46c79a83d4be",
   "metadata": {},
   "source": [
    "Download the homework data from Moodle. In the archive, you will find two files: Two German tokenized\n",
    "text with 50K lines each. Each line consists of a sentence; special tokens have been added at the beginning\n",
    "and at the end.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1831ab3-4dbd-441f-a89d-93407b0cacc1",
   "metadata": {},
   "source": [
    "Example: %^% %^% Leder : Vielleicht ringt Normann nur um Anerkennung . %$% %$%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4b5e3f-847f-4b72-b038-3974cb522094",
   "metadata": {},
   "source": [
    "This sentence has 9 tokens, 10 bigrams and 11 trigrams: note that the special tokens %^% and %$% are\n",
    "only considered if needed. Tokens are separated by a space character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06b2eeb9-3d55-4e7d-9709-4c1b2c446e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "Trotzdem ist die Aufgabe machbar \" , sagte Hertha-Coach Falko Götz .\n",
      "Zu akuten zerebralen Ereignissen kam es bei 2 Patienten , was im Rahmen der bisherigen Studien liegt .\n"
     ]
    }
   ],
   "source": [
    "# read the files\n",
    "import os\n",
    "from random import randint\n",
    "prep = True\n",
    "data_dir = './de_text'\n",
    "train_file = 'de_text.test'\n",
    "test_file = 'de_text.train'\n",
    "trainfile = os.path.join(data_dir, train_file)\n",
    "testfile = os.path.join(data_dir, test_file)\n",
    "def preprocess(to_process, start_pattern, end_pattern):\n",
    "    t = to_process\n",
    "    if start_pattern:\n",
    "        t = t.strip(start_pattern)\n",
    "    if end_pattern:\n",
    "        t = t.strip(end_pattern)\n",
    "    return t\n",
    "\n",
    "def read_and_prep(path_to_file, prep = True):\n",
    "    with open(path_to_file, encoding = 'utf-8') as f:\n",
    "        corp = [x.strip('\\n') for x in f]\n",
    "    if prep:\n",
    "        corp = [preprocess(x,'%^% %^%', '%$% %$%') for x in corp]\n",
    "    return corp\n",
    "\n",
    "def get_sets():\n",
    "    return read_and_prep(trainfile), read_and_prep(testfile)\n",
    "\n",
    "train, test = get_sets()\n",
    "print(len(train))\n",
    "print(train[randint(0, len(train)-1)])\n",
    "print(train[randint(0, len(train)-1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b021e7-c430-430e-938c-7d94ba478829",
   "metadata": {},
   "source": [
    "### a) List the 20 most frequent words from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c80a495-7648-4e0e-a8da-e5c9bdda686b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 most frequent token are [('.', 51601), (',', 43454), ('der', 24425), ('die', 23464), ('und', 15684), ('in', 13147), ('\"', 12934), ('den', 9621), ('von', 7450), ('zu', 7014), ('das', 6669), ('mit', 6434), ('sich', 6050), ('ist', 5955), ('auf', 5726), ('für', 5572), ('nicht', 5542), ('im', 5526), ('Die', 5429), ('des', 5186)]\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "train, test = get_sets()\n",
    "\n",
    "def get_n_grams(n, corpus):\n",
    "    ngrams = {}\n",
    "    for doc in corpus:\n",
    "        #get token\n",
    "        tokens = doc.split(\" \")\n",
    "        # pad it\n",
    "        for i in range(n-1):\n",
    "            tokens.insert(0, '<bos>')\n",
    "            tokens.append('<eos>')\n",
    "            \n",
    "        # take n grams, starting at positon n-1, as it is the first non-pad token\n",
    "        for i in range(n-1, len(tokens)):\n",
    "            n_gram = tokens[i]\n",
    "            \n",
    "            # build n_grams by appending n-1 token from the left\n",
    "            for j in range(1, n):\n",
    "                n_gram = tokens[i-j] + \" \" + n_gram\n",
    "            \n",
    "            try:\n",
    "                ngrams[n_gram] +=1\n",
    "            except KeyError:\n",
    "                ngrams[n_gram] = 1\n",
    "    \n",
    "    return ngrams\n",
    "\n",
    "def get_n_most_freq(vocabulary, n=10):\n",
    "    return sorted(vocabulary.items(), key = itemgetter(1), reverse=True)[:n]\n",
    "    \n",
    "train_vocab = get_n_grams(1, train)\n",
    "n = 20\n",
    "most_f = get_n_most_freq(train_vocab, n)\n",
    "print(f\"Top {n} most frequent token are {str(most_f)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6871be-735d-4f83-b299-f06f326d6738",
   "metadata": {},
   "source": [
    "### b) Compute the percentage of tokens in the test data that have not been seen in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aa21174-e53d-436b-9553-429714a8cb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 107126 unique token in the test corpus. 61854 of them are unique or 57.74% considering the vocabulary size\n"
     ]
    }
   ],
   "source": [
    "train, test = get_sets()\n",
    "\n",
    "test_vocab = get_n_grams(1, test)\n",
    "\n",
    "def get_unique_keys(dict1, dict2):\n",
    "    unique = 0\n",
    "    for k in dict1.keys():\n",
    "        if k not in dict2:\n",
    "            unique += 1\n",
    "    return unique\n",
    "\n",
    "unique_keys_in_test = get_unique_keys(test_vocab, train_vocab)\n",
    "print(f\"There are {len(test_vocab)} unique token in the test corpus. {unique_keys_in_test} of them are unique or {round(100 * unique_keys_in_test/len(test_vocab), 2)}% considering the vocabulary size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953a1ada-4b89-42c7-a686-3d2655eb9d14",
   "metadata": {},
   "source": [
    "Counting all token in the corpus, the counts differ a lot:   \n",
    "\n",
    "Number of unigrams in test-data: 907335  \n",
    "Number token in test, but not in training: 67300\n",
    "\n",
    "Percentage not seen in train: 0.07417326566262736"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58094fd-a8ab-47c3-8a91-9eece4863234",
   "metadata": {},
   "source": [
    "### c) List the 20 most frequent bigrams from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7d54b3c-be70-4870-bb9c-aaec38ae34ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 481575 unique bigrams in the train corpus. The 20 most frequent are [('. <eos>', 47153), ('<bos> Die', 4789), (', die', 4052), ('<bos> Der', 2761), ('in der', 2482), ('<bos> Das', 2014), ('<bos> \"', 1895), (', der', 1883), ('\" ,', 1807), (', dass', 1622), (', daß', 1380), ('in den', 1364), ('für die', 1339), ('<bos> In', 1291), ('? <eos>', 1267), ('\" <eos>', 1134), ('. \"', 1133), ('werden .', 1087), (', das', 1004), ('<bos> Und', 968)]\n",
      "There are 484359 unique bigrams in the train corpus. The 20 most frequent are [('. <eos>', 47095), ('<bos> Die', 5011), (', die', 3900), ('<bos> Der', 2749), ('in der', 2470), ('<bos> Das', 1957), (', der', 1879), ('\" ,', 1858), ('<bos> \"', 1850), (', dass', 1599), ('<bos> In', 1376), ('in den', 1362), (', daß', 1346), ('für die', 1346), ('? <eos>', 1240), ('\" <eos>', 1174), ('. \"', 1159), ('werden .', 1077), (', das', 1025), ('\" .', 918)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train, test = get_sets()\n",
    "n = 20\n",
    "\n",
    "train_bigrams = get_n_grams(2, train)\n",
    "test_bigrams = get_n_grams(2, test)\n",
    "\n",
    "most_freq_train_bi = get_n_most_freq(train_bigrams, n)\n",
    "most_freq_test_bi = get_n_most_freq(test_bigrams, n)\n",
    "\n",
    "print(f\"There are {len(train_bigrams)} unique bigrams in the train corpus. The {n} most frequent are {str(most_freq_train_bi)}\")\n",
    "print(f\"There are {len(test_bigrams)} unique bigrams in the train corpus. The {n} most frequent are {str(most_freq_test_bi)}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a06fef-96db-4301-afb8-22322a18b8c0",
   "metadata": {},
   "source": [
    "### d) Compute the percentage of bigrams in the test data that have not been seen in the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38cf7306-0ec2-4998-b181-ea9e749c06e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique token in test: 377808. Token in test: 484359. Percentage: 78.0%\n"
     ]
    }
   ],
   "source": [
    "train, test = get_sets()\n",
    "\n",
    "train_bigrams = get_n_grams(2, train)\n",
    "test_bigrams = get_n_grams(2, test)\n",
    "\n",
    "unique_keys_in_test = get_unique_keys(test_bigrams, train_bigrams)\n",
    "\n",
    "\n",
    "print(f\"Unique token in test: {unique_keys_in_test}. Token in test: {len(test_bigrams)}. Percentage: {round(100* unique_keys_in_test / len(test_bigrams), 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaf97cc-9814-4512-a31f-3b2780571065",
   "metadata": {},
   "source": [
    "Consindering all bigrams in corpus, not only unique the numbers are as follows:  \n",
    "Number of bigrams in test-data: 957335  \n",
    "Number token in test, but not in training: 391930  \n",
    "\n",
    "Percentage not seen in train: 0.40939691957360796"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4455ae0e-eff0-415f-acaa-40301448f3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = get_sets()\n",
    "\n",
    "train_trigrams = get_n_grams(3, train)\n",
    "test_trigrams = get_n_grams(3, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c22d32c-6b74-4291-bb3e-32d6d1a3f4d1",
   "metadata": {},
   "source": [
    "## e) Compute the percentage of trigrams in the test data that have not been seen in the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb0b5e2a-08c2-49c0-b9b8-c285e27a6501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 771937 unique trigrams in the train corpus. The 20 most frequent are [('. <eos> <eos>', 47153), ('<bos> <bos> Die', 4789), ('<bos> <bos> Der', 2761), ('<bos> <bos> Das', 2014), ('<bos> <bos> \"', 1895), ('<bos> <bos> In', 1291), ('? <eos> <eos>', 1267), ('\" <eos> <eos>', 1134), ('werden . <eos>', 1059), ('. \" <eos>', 1005), ('<bos> <bos> Und', 968), ('<bos> <bos> Es', 879), ('\" . <eos>', 868), ('<bos> <bos> Er', 858), ('<bos> <bos> Sie', 822), ('<bos> <bos> Im', 816), ('<bos> <bos> Ein', 707), ('<bos> <bos> Auch', 704), ('<bos> <bos> Nach', 658), ('<bos> <bos> Doch', 611)]\n",
      "\n",
      "There are 775702 unique trigrams in the train corpus. The 20 most frequent are [('. <eos> <eos>', 47095), ('<bos> <bos> Die', 5011), ('<bos> <bos> Der', 2749), ('<bos> <bos> Das', 1957), ('<bos> <bos> \"', 1850), ('<bos> <bos> In', 1376), ('? <eos> <eos>', 1240), ('\" <eos> <eos>', 1174), ('werden . <eos>', 1045), ('. \" <eos>', 1039), ('\" . <eos>', 905), ('<bos> <bos> Und', 872), ('<bos> <bos> Es', 863), ('<bos> <bos> Er', 802), ('<bos> <bos> Im', 784), ('<bos> <bos> Sie', 770), ('<bos> <bos> Auch', 715), ('<bos> <bos> Ein', 672), ('<bos> <bos> Nach', 642), ('<bos> <bos> Mit', 616)]\n",
      "\n",
      "Unique token in test: 702953. Token in test: 775702. Percentage: 90.62%\n"
     ]
    }
   ],
   "source": [
    "most_freq_train_tri = get_n_most_freq(train_trigrams, n)\n",
    "most_freq_test_tri = get_n_most_freq(test_trigrams, n)\n",
    "unique_keys_in_test_tri = get_unique_keys(test_trigrams, train_trigrams)\n",
    "print(f\"There are {len(train_trigrams)} unique trigrams in the train corpus. The {n} most frequent are {str(most_freq_train_tri)}\")\n",
    "print()\n",
    "print(f\"There are {len(test_trigrams)} unique trigrams in the train corpus. The {n} most frequent are {str(most_freq_test_tri)}\")\n",
    "print()\n",
    "print(f\"Unique token in test: {unique_keys_in_test_tri}. Token in test: {len(test_trigrams)}. Percentage: {round(100* unique_keys_in_test_tri / len(test_trigrams), 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba278f2c-aa7a-49e2-8f21-bca7227c0ea7",
   "metadata": {},
   "source": [
    "Again, counting all trigrams, the numbers change:  \n",
    "Number of trigrams in train-data: 1011850  \n",
    "Number of trigrams in test-data: 1007335  \n",
    "Number token in test, but not in training: 715001  \n",
    "\n",
    "Percentage not seen in train: 0.7097946561967965"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9698716e-c0c5-4527-ad03-061d86146225",
   "metadata": {},
   "source": [
    "### f) How many sentences in the test data are estimated to have zero probability by an MLE bigram model from the training data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56aa4094-2a33-4537-b18b-9402e6a1b71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_n_minus_one_gram(ngram):\n",
    "    # split token at \" \", remove last and concat\n",
    "    token = ngram.split(\" \")\n",
    "    token = token[:-1]\n",
    "    return \" \".join(token)\n",
    "\n",
    "def create_mle_model(corpus, n=2):\n",
    "    n_gram_counts = get_n_grams(n, corpus)\n",
    "    mle ={}\n",
    "    if n ==1:\n",
    "        total_token = sum(n_gram_counts.values())\n",
    "        for token in n_gram_counts:\n",
    "            mle[token] = n_gram_counts[token] / total_token\n",
    "        return mle\n",
    "    n_minus_one_gram_count = get_n_grams(n-1, corpus)\n",
    "    \n",
    "    if n==2:\n",
    "        # for unigrams, add padding symbolds\n",
    "        n_minus_one_gram_count[\"<bos>\"] = len(corpus)\n",
    "        n_minus_one_gram_count[\"<eos>\"] = len(corpus)\n",
    "    if n==3:\n",
    "        n_minus_one_gram_count[\"<bos> <bos>\"] = len(corpus)\n",
    "        n_minus_one_gram_count[\"<eos> <eos>\"] = len(corpus)\n",
    "    \n",
    "    \n",
    "    for ngram in n_gram_counts.keys():\n",
    "        c_of_w = n_gram_counts[ngram]\n",
    "        \n",
    "        n_minus_one_gram = build_n_minus_one_gram(ngram)\n",
    "        c_of_w_minus_one = n_minus_one_gram_count[n_minus_one_gram]\n",
    "        mle[ngram] = c_of_w / c_of_w_minus_one\n",
    "    return mle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "316f502d-15f6-44ac-9e51-8250369206bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49710 of 50000 documents have 0 proba (or 99.42%)\n"
     ]
    }
   ],
   "source": [
    "# calc mle for test corpus\n",
    "def get_bigrams(document):\n",
    "    bigrams = []\n",
    "    tokens = document.split(\" \")\n",
    "    # add padding bigrams\n",
    "    bigrams.append(\"<bos> \" + tokens[0])\n",
    "    bigrams.append(tokens[-1] + \" <eos>\")\n",
    "    if len(tokens)==1:\n",
    "        return bigrams\n",
    "    for i in range(1, len(tokens)):\n",
    "        bigrams.append(tokens[i-1] + \" \" + tokens[i])\n",
    "    return bigrams\n",
    "\n",
    "def get_ngram_proba(ngrams, model):\n",
    "    assert type(ngrams) == list\n",
    "    assert type(model) == dict\n",
    "    try:\n",
    "        proba = model[ngrams[0]]\n",
    "        for ngram in ngrams[1:]:\n",
    "            proba = proba * model[ngram]\n",
    "        return proba\n",
    "    except KeyError:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def eval_mle_model(train, test):\n",
    "    model = create_mle_model(train, 2)\n",
    "    counter, zero_probas = 0,0\n",
    "    for doc in test:\n",
    "        bigrams = get_bigrams(doc)\n",
    "        proba = get_ngram_proba(bigrams, model)\n",
    "        counter +=1\n",
    "        if proba == 0:\n",
    "            zero_probas +=1\n",
    "    return counter, zero_probas\n",
    "\n",
    "train, test = get_sets()\n",
    "docs, zeros = eval_mle_model(train, test)\n",
    "print(f\"{zeros} of {docs} documents have 0 proba (or {round(100*zeros/docs, 2)}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fdfbfd-d7d7-4cbb-ac93-7c8f86ccebe1",
   "metadata": {},
   "source": [
    "### g) Give the probabilities of the first 3 sentences from the test data, using a linear combination of 0-gram, unigram, bigram and trigram model with λ0 = 1.0×10−10, λ1 = 0.01, λ2 = 0.2, λ3 = 1−(λ0+λ1+λ2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94789ca4-94d4-4a61-80ac-7a87bcd03251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 0\n",
      "uni prob is 3.821599123355931e-45 for: Die Preise für ein Einzelzimmer liegen hier zwischen 129 und 149 DM .\n",
      "0 3.821599123355931e-45 0 0\n",
      "0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "def transform_sent_to_ngram(sentence, n):\n",
    "    for variablerCounter in range(n-1):\n",
    "        sentence = \"<bos> \" + sentence\n",
    "        sentence = sentence + \" <eos>\"\n",
    "    tokens = sentence.split(\" \")\n",
    "    if n==1:\n",
    "        return tokens\n",
    "    n_grams =[]\n",
    "    for i in range(n-1, len(tokens)):\n",
    "        n_gram = tokens[i]\n",
    "        for j in range(1, n):\n",
    "            #append left\n",
    "            n_gram = tokens[i-j] + \" \" + n_gram\n",
    "        n_grams.append(n_gram)\n",
    "    return n_grams\n",
    "        \n",
    "def combine_linear(val0, val1, val2,val3):\n",
    "    print(val0, val1, val2, val3)\n",
    "    d0 = 1.0*10**-10\n",
    "    d1 = 0.01\n",
    "    d2 = 0.2\n",
    "    d3 = 1-d0-d1-d2\n",
    "    return d0*val0 + d1 * val1 + d2*val2 + d3*val3\n",
    "\n",
    "#wtf is 0 - gram? equal proba for each? Letters?\n",
    "def get_zero_gram_proba(doc, corp):\n",
    "    # TODO insert somethin useful here maybe, I just use even proba for each sentence in test set, otherwise 0\n",
    "    if doc in corp:\n",
    "        return 1 / len(corp)\n",
    "    return 0\n",
    "\n",
    "def lin_combo_model():\n",
    "    train, test = get_sets()\n",
    "    # get the mle models trained with train\n",
    "    unigram_model = create_mle_model(train,1)\n",
    "    bigram_model = create_mle_model(train,2)\n",
    "    trigram_model = create_mle_model(train,3)\n",
    "\n",
    "    #just take first three examples\n",
    "    test = test[:3]\n",
    "    probas = []\n",
    "    for doc in test:\n",
    "        unis = transform_sent_to_ngram(doc, 1)\n",
    "        bi = transform_sent_to_ngram(doc, 2)\n",
    "        tri = transform_sent_to_ngram(doc, 3)\n",
    "        \n",
    "        \n",
    "        zeroprob = get_zero_gram_proba(doc, train)\n",
    "        if zeroprob != 0:\n",
    "            print(f\"Zero prob is {zeroprob} for: {doc}\")\n",
    "        uniprob = get_ngram_proba(unis, unigram_model)\n",
    "        if uniprob != 0:\n",
    "            print(f\"uni prob is {uniprob} for: {doc}\")\n",
    "        biprob = get_ngram_proba(bi, bigram_model)\n",
    "        if biprob != 0:\n",
    "            print(f\"Bi prob is {biprob} for: {doc}\")\n",
    "        triprob = get_ngram_proba(tri, trigram_model)\n",
    "        if zeroprob != 0:\n",
    "            print(f\"tri prob is {triprob} for: {doc}\")\n",
    "        \n",
    "        res = combine_linear(zeroprob, uniprob, biprob, triprob)\n",
    "        \n",
    "        probas.append((doc, res))\n",
    "    return probas\n",
    "\n",
    "result = lin_combo_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e227f7ac-10d8-4b0e-8039-b4cf0b4b5aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Stanczyk nannte es beunruhigend , daß die Bundesregierung in dieser Frage bislang nicht einmal informell Kontakt zur polnischen Regierung gesucht habe .', 0.0), ('Die Preise für ein Einzelzimmer liegen hier zwischen 129 und 149 DM .', 3.8215991233559306e-47), ('Leder : Vielleicht ringt Normann nur um Anerkennung .', 0.0)]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b35a911-e170-4a93-8056-ec166bc936dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
