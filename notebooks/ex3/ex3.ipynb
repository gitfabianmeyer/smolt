{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76213385-11a1-44a8-b79f-f5bd1aeb2387",
   "metadata": {},
   "source": [
    "#### Note: Might use many resources, as I calc everything new for each cell / task to ensure partial usability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5113fedb-9445-49f9-8192-99132b982ae5",
   "metadata": {},
   "source": [
    "![1)](part1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28fcb59-c45c-4e29-8ebb-de2507b7599c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b782f73c-9519-4001-a494-46c79a83d4be",
   "metadata": {},
   "source": [
    "Download the homework data from Moodle. In the archive, you will find two files: Two German tokenized\n",
    "text with 50K lines each. Each line consists of a sentence; special tokens have been added at the beginning\n",
    "and at the end.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1831ab3-4dbd-441f-a89d-93407b0cacc1",
   "metadata": {},
   "source": [
    "Example: %^% %^% Leder : Vielleicht ringt Normann nur um Anerkennung . %$% %$%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4b5e3f-847f-4b72-b038-3974cb522094",
   "metadata": {},
   "source": [
    "This sentence has 9 tokens, 10 bigrams and 11 trigrams: note that the special tokens %^% and %$% are\n",
    "only considered if needed. Tokens are separated by a space character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06b2eeb9-3d55-4e7d-9709-4c1b2c446e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "Auch bei der Flugsicherung Sky Guide gab es zum Unglückszeitpunkt zahlreiche Sicherheitslücken .\n",
      "Ungarn wird Ende März seine Soldaten abziehen .\n"
     ]
    }
   ],
   "source": [
    "# read the files\n",
    "import os\n",
    "from random import randint\n",
    "prep = True\n",
    "data_dir = './de_text'\n",
    "train_file = 'de_text.test'\n",
    "test_file = 'de_text.train'\n",
    "trainfile = os.path.join(data_dir, train_file)\n",
    "testfile = os.path.join(data_dir, test_file)\n",
    "def preprocess(to_process, start_pattern, end_pattern):\n",
    "    t = to_process\n",
    "    if start_pattern:\n",
    "        t = t.strip(start_pattern)\n",
    "    if end_pattern:\n",
    "        t = t.strip(end_pattern)\n",
    "    return t\n",
    "\n",
    "def read_and_prep(path_to_file, prep = True):\n",
    "    with open(path_to_file, encoding = 'utf-8') as f:\n",
    "        corp = [x.strip('\\n') for x in f]\n",
    "    if prep:\n",
    "        corp = [preprocess(x,'%^% %^%', '%$% %$%') for x in corp]\n",
    "    return corp\n",
    "\n",
    "def get_sets():\n",
    "    return read_and_prep(trainfile), read_and_prep(testfile)\n",
    "\n",
    "train, test = get_sets()\n",
    "print(len(train))\n",
    "print(train[randint(0, len(train)-1)])\n",
    "print(train[randint(0, len(train)-1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b021e7-c430-430e-938c-7d94ba478829",
   "metadata": {},
   "source": [
    "### a) List the 20 most frequent words from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c80a495-7648-4e0e-a8da-e5c9bdda686b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 most frequent token are [('.', 51601), (',', 43454), ('der', 24425), ('die', 23464), ('und', 15684), ('in', 13147), ('\"', 12934), ('den', 9621), ('von', 7450), ('zu', 7014), ('das', 6669), ('mit', 6434), ('sich', 6050), ('ist', 5955), ('auf', 5726), ('für', 5572), ('nicht', 5542), ('im', 5526), ('Die', 5429), ('des', 5186)]\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "train, test = get_sets()\n",
    "\n",
    "def get_n_grams(n, corpus):\n",
    "    ngrams = {}\n",
    "    for doc in corpus:\n",
    "        #get token\n",
    "        tokens = doc.split(\" \")\n",
    "        # pad it\n",
    "        for i in range(n-1):\n",
    "            tokens.insert(0, '<bos>')\n",
    "            tokens.append('<eos>')\n",
    "            \n",
    "        # take n grams, starting at positon n-1, as it is the first non-pad token\n",
    "        for i in range(n-1, len(tokens)):\n",
    "            n_gram = tokens[i]\n",
    "            \n",
    "            # build n_grams by appending n-1 token from the left\n",
    "            for j in range(1, n):\n",
    "                n_gram = tokens[i-j] + \" \" + n_gram\n",
    "            \n",
    "            try:\n",
    "                ngrams[n_gram] +=1\n",
    "            except KeyError:\n",
    "                ngrams[n_gram] = 1\n",
    "    \n",
    "    return ngrams\n",
    "\n",
    "def get_n_most_freq(vocabulary, n=10):\n",
    "    return sorted(vocabulary.items(), key = itemgetter(1), reverse=True)[:n]\n",
    "    \n",
    "train_vocab = get_n_grams(1, train)\n",
    "n = 20\n",
    "most_f = get_n_most_freq(train_vocab, n)\n",
    "print(f\"Top {n} most frequent token are {str(most_f)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6871be-735d-4f83-b299-f06f326d6738",
   "metadata": {},
   "source": [
    "### b) Compute the percentage of tokens in the test data that have not been seen in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aa21174-e53d-436b-9553-429714a8cb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 107126 unique token in the test corpus. 61854 of them are unique or 57.74%\n"
     ]
    }
   ],
   "source": [
    "train, test = get_sets()\n",
    "\n",
    "test_vocab = get_n_grams(1, test)\n",
    "\n",
    "def get_unique_keys(dict1, dict2):\n",
    "    unique = 0\n",
    "    for k in dict1.keys():\n",
    "        if k not in dict2:\n",
    "            unique += 1\n",
    "    return unique\n",
    "\n",
    "unique_keys_in_test = get_unique_keys(test_vocab, train_vocab)\n",
    "print(f\"There are {len(test_vocab)} unique token in the test corpus. {unique_keys_in_test} of them are unique or {round(100 * unique_keys_in_test/len(test_vocab), 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58094fd-a8ab-47c3-8a91-9eece4863234",
   "metadata": {},
   "source": [
    "### c) List the 20 most frequent bigrams from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7d54b3c-be70-4870-bb9c-aaec38ae34ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 481575 unique bigrams in the train corpus. The 20 most frequent are [('. <eos>', 47153), ('<bos> Die', 4789), (', die', 4052), ('<bos> Der', 2761), ('in der', 2482), ('<bos> Das', 2014), ('<bos> \"', 1895), (', der', 1883), ('\" ,', 1807), (', dass', 1622), (', daß', 1380), ('in den', 1364), ('für die', 1339), ('<bos> In', 1291), ('? <eos>', 1267), ('\" <eos>', 1134), ('. \"', 1133), ('werden .', 1087), (', das', 1004), ('<bos> Und', 968)]\n",
      "There are 484359 unique bigrams in the train corpus. The 20 most frequent are [('. <eos>', 47095), ('<bos> Die', 5011), (', die', 3900), ('<bos> Der', 2749), ('in der', 2470), ('<bos> Das', 1957), (', der', 1879), ('\" ,', 1858), ('<bos> \"', 1850), (', dass', 1599), ('<bos> In', 1376), ('in den', 1362), (', daß', 1346), ('für die', 1346), ('? <eos>', 1240), ('\" <eos>', 1174), ('. \"', 1159), ('werden .', 1077), (', das', 1025), ('\" .', 918)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train, test = get_sets()\n",
    "n = 20\n",
    "\n",
    "train_bigrams = get_n_grams(2, train)\n",
    "test_bigrams = get_n_grams(2, test)\n",
    "\n",
    "most_freq_train_bi = get_n_most_freq(train_bigrams, n)\n",
    "most_freq_test_bi = get_n_most_freq(test_bigrams, n)\n",
    "\n",
    "print(f\"There are {len(train_bigrams)} unique bigrams in the train corpus. The {n} most frequent are {str(most_freq_train_bi)}\")\n",
    "print(f\"There are {len(test_bigrams)} unique bigrams in the train corpus. The {n} most frequent are {str(most_freq_test_bi)}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a06fef-96db-4301-afb8-22322a18b8c0",
   "metadata": {},
   "source": [
    "### d) Compute the percentage of bigrams in the test data that have not been seen in the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38cf7306-0ec2-4998-b181-ea9e749c06e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique token in test: 377808. Token in test: 484359. Percentage: 78.0%\n"
     ]
    }
   ],
   "source": [
    "train, test = get_sets()\n",
    "\n",
    "train_bigrams = get_n_grams(2, train)\n",
    "test_bigrams = get_n_grams(2, test)\n",
    "\n",
    "unique_keys_in_test = get_unique_keys(test_bigrams, train_bigrams)\n",
    "\n",
    "\n",
    "print(f\"Unique token in test: {unique_keys_in_test}. Token in test: {len(test_bigrams)}. Percentage: {round(100* unique_keys_in_test / len(test_bigrams), 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4455ae0e-eff0-415f-acaa-40301448f3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = get_sets()\n",
    "\n",
    "train_trigrams = get_n_grams(3, train)\n",
    "test_trigrams = get_n_grams(3, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb0b5e2a-08c2-49c0-b9b8-c285e27a6501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 771937 unique trigrams in the train corpus. The 20 most frequent are [('. <eos> <eos>', 47153), ('<bos> <bos> Die', 4789), ('<bos> <bos> Der', 2761), ('<bos> <bos> Das', 2014), ('<bos> <bos> \"', 1895), ('<bos> <bos> In', 1291), ('? <eos> <eos>', 1267), ('\" <eos> <eos>', 1134), ('werden . <eos>', 1059), ('. \" <eos>', 1005), ('<bos> <bos> Und', 968), ('<bos> <bos> Es', 879), ('\" . <eos>', 868), ('<bos> <bos> Er', 858), ('<bos> <bos> Sie', 822), ('<bos> <bos> Im', 816), ('<bos> <bos> Ein', 707), ('<bos> <bos> Auch', 704), ('<bos> <bos> Nach', 658), ('<bos> <bos> Doch', 611)]\n",
      "\n",
      "There are 775702 unique trigrams in the train corpus. The 20 most frequent are [('. <eos> <eos>', 47095), ('<bos> <bos> Die', 5011), ('<bos> <bos> Der', 2749), ('<bos> <bos> Das', 1957), ('<bos> <bos> \"', 1850), ('<bos> <bos> In', 1376), ('? <eos> <eos>', 1240), ('\" <eos> <eos>', 1174), ('werden . <eos>', 1045), ('. \" <eos>', 1039), ('\" . <eos>', 905), ('<bos> <bos> Und', 872), ('<bos> <bos> Es', 863), ('<bos> <bos> Er', 802), ('<bos> <bos> Im', 784), ('<bos> <bos> Sie', 770), ('<bos> <bos> Auch', 715), ('<bos> <bos> Ein', 672), ('<bos> <bos> Nach', 642), ('<bos> <bos> Mit', 616)]\n",
      "\n",
      "Unique token in test: 702953. Token in test: 775702. Percentage: 90.62%\n"
     ]
    }
   ],
   "source": [
    "most_freq_train_tri = get_n_most_freq(train_trigrams, n)\n",
    "most_freq_test_tri = get_n_most_freq(test_trigrams, n)\n",
    "unique_keys_in_test_tri = get_unique_keys(test_trigrams, train_trigrams)\n",
    "print(f\"There are {len(train_trigrams)} unique trigrams in the train corpus. The {n} most frequent are {str(most_freq_train_tri)}\")\n",
    "print()\n",
    "print(f\"There are {len(test_trigrams)} unique trigrams in the train corpus. The {n} most frequent are {str(most_freq_test_tri)}\")\n",
    "print()\n",
    "print(f\"Unique token in test: {unique_keys_in_test_tri}. Token in test: {len(test_trigrams)}. Percentage: {round(100* unique_keys_in_test_tri / len(test_trigrams), 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9698716e-c0c5-4527-ad03-061d86146225",
   "metadata": {},
   "source": [
    "### f) How many sentences in the test data are estimated to have zero probability by an MLE bigram model from the training data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e33300dc-5832-4111-886f-bfafdb9742e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wort eins zwei'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"wort eins zwei drei\"\n",
    "b = a.split(\" \")\n",
    "b = b[:-1]\n",
    "\" \".join(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56aa4094-2a33-4537-b18b-9402e6a1b71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_n_minus_one_gram(ngram):\n",
    "    # split token at \" \", remove last and concat\n",
    "    token = ngram.split(\" \")\n",
    "    token = token[:-1]\n",
    "    return \" \".join(token)\n",
    "\n",
    "def create_mle_model(corpus, n=2):\n",
    "    n_gram_counts = get_n_grams(n, corpus)\n",
    "    mle ={}\n",
    "    if n ==1:\n",
    "        total_token = sum(n_gram_counts.values())\n",
    "        for token in n_gram_counts:\n",
    "            mle[token] = n_gram_counts[token] / total_token\n",
    "        return mle\n",
    "    n_minus_one_gram_count = get_n_grams(n-1, corpus)\n",
    "    \n",
    "    if n==2:\n",
    "        # for unigrams, add padding symbolds\n",
    "        n_minus_one_gram_count[\"<bos>\"] = len(corpus)\n",
    "        n_minus_one_gram_count[\"<eos>\"] = len(corpus)\n",
    "    if n==3:\n",
    "        n_minus_one_gram_count[\"<bos> <bos>\"] = len(corpus)\n",
    "        n_minus_one_gram_count[\"<eos> <eos>\"] = len(corpus)\n",
    "    \n",
    "    \n",
    "    for ngram in n_gram_counts.keys():\n",
    "        c_of_w = n_gram_counts[ngram]\n",
    "        \n",
    "        n_minus_one_gram = build_n_minus_one_gram(ngram)\n",
    "        c_of_w_minus_one = n_minus_one_gram_count[n_minus_one_gram]\n",
    "        mle[ngram] = c_of_w / c_of_w_minus_one\n",
    "    return mle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "316f502d-15f6-44ac-9e51-8250369206bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49710 of 50000 documents have 0 proba (or 99.42%)\n"
     ]
    }
   ],
   "source": [
    "# calc mle for test corpus\n",
    "def get_bigrams(document):\n",
    "    bigrams = []\n",
    "    tokens = document.split(\" \")\n",
    "    # add padding bigrams\n",
    "    bigrams.append(\"<bos> \" + tokens[0])\n",
    "    bigrams.append(tokens[-1] + \" <eos>\")\n",
    "    if len(tokens)==1:\n",
    "        return bigrams\n",
    "    for i in range(1, len(tokens)):\n",
    "        bigrams.append(tokens[i-1] + \" \" + tokens[i])\n",
    "    return bigrams\n",
    "\n",
    "def get_ngram_proba(ngrams, model):\n",
    "    assert type(ngrams) == list\n",
    "    assert type(model) == dict\n",
    "    try:\n",
    "        proba = model[ngrams[0]]\n",
    "        for ngram in ngrams[1:]:\n",
    "            proba = proba * model[ngram]\n",
    "        return proba\n",
    "    except KeyError:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def eval_mle_model(train, test):\n",
    "    model = create_mle_model(train, 2)\n",
    "    counter, zero_probas = 0,0\n",
    "    for doc in test:\n",
    "        bigrams = get_bigrams(doc)\n",
    "        proba = get_ngram_proba(bigrams, model)\n",
    "        counter +=1\n",
    "        if proba == 0:\n",
    "            zero_probas +=1\n",
    "    return counter, zero_probas\n",
    "\n",
    "train, test = get_sets()\n",
    "docs, zeros = eval_mle_model(train, test)\n",
    "print(f\"{zeros} of {docs} documents have 0 proba (or {round(100*zeros/docs, 2)}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fdfbfd-d7d7-4cbb-ac93-7c8f86ccebe1",
   "metadata": {},
   "source": [
    "### g) Give the probabilities of the first 3 sentences from the test data, using a linear combination of 0-gram, unigram, bigram and trigram model with λ0 = 1.0×10−10, λ1 = 0.01, λ2 = 0.2, λ3 = 1−(λ0+λ1+λ2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94789ca4-94d4-4a61-80ac-7a87bcd03251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 0\n",
      "uni prob is 3.821599123355931e-45 for: Die Preise für ein Einzelzimmer liegen hier zwischen 129 und 149 DM .\n",
      "0 3.821599123355931e-45 0 0\n",
      "0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "def transform_sent_to_ngram(sentence, n):\n",
    "    for variablerCounter in range(n-1):\n",
    "        sentence = \"<bos> \" + sentence\n",
    "        sentence = sentence + \" <eos>\"\n",
    "    tokens = sentence.split(\" \")\n",
    "    if n==1:\n",
    "        return tokens\n",
    "    n_grams =[]\n",
    "    for i in range(n-1, len(tokens)):\n",
    "        n_gram = tokens[i]\n",
    "        for j in range(1, n):\n",
    "            #append left\n",
    "            n_gram = tokens[i-j] + \" \" + n_gram\n",
    "        n_grams.append(n_gram)\n",
    "    return n_grams\n",
    "        \n",
    "def combine_linear(val0, val1, val2,val3):\n",
    "    print(val0, val1, val2, val3)\n",
    "    d0 = 1.0*10**-10\n",
    "    d1 = 0.01\n",
    "    d2 = 0.2\n",
    "    d3 = 1-d0-d1-d2\n",
    "    return d0*val0 + d1 * val1 + d2*val2 + d3*val3\n",
    "\n",
    "#wtf is 0 - gram? equal proba for each? Letters?\n",
    "def get_zero_gram_proba(doc, corp):\n",
    "    # TODO insert somethin useful here maybe, I just use even proba for each sentence in test set, otherwise 0\n",
    "    if doc in corp:\n",
    "        return 1 / len(corp)\n",
    "    return 0\n",
    "\n",
    "def lin_combo_model():\n",
    "    train, test = get_sets()\n",
    "    # get the mle models trained with train\n",
    "    unigram_model = create_mle_model(train,1)\n",
    "    bigram_model = create_mle_model(train,2)\n",
    "    trigram_model = create_mle_model(train,3)\n",
    "\n",
    "    #just take first three examples\n",
    "    test = test[:3]\n",
    "    probas = []\n",
    "    for doc in test:\n",
    "        unis = transform_sent_to_ngram(doc, 1)\n",
    "        bi = transform_sent_to_ngram(doc, 2)\n",
    "        tri = transform_sent_to_ngram(doc, 3)\n",
    "        \n",
    "        \n",
    "        zeroprob = get_zero_gram_proba(doc, train)\n",
    "        if zeroprob != 0:\n",
    "            print(f\"Zero prob is {zeroprob} for: {doc}\")\n",
    "        uniprob = get_ngram_proba(unis, unigram_model)\n",
    "        if uniprob != 0:\n",
    "            print(f\"uni prob is {uniprob} for: {doc}\")\n",
    "        biprob = get_ngram_proba(bi, bigram_model)\n",
    "        if biprob != 0:\n",
    "            print(f\"Bi prob is {biprob} for: {doc}\")\n",
    "        triprob = get_ngram_proba(tri, trigram_model)\n",
    "        if zeroprob != 0:\n",
    "            print(f\"tri prob is {triprob} for: {doc}\")\n",
    "        \n",
    "        res = combine_linear(zeroprob, uniprob, biprob, triprob)\n",
    "        \n",
    "        probas.append((doc, res))\n",
    "    return probas\n",
    "\n",
    "result = lin_combo_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e227f7ac-10d8-4b0e-8039-b4cf0b4b5aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Stanczyk nannte es beunruhigend , daß die Bundesregierung in dieser Frage bislang nicht einmal informell Kontakt zur polnischen Regierung gesucht habe .', 0.0), ('Die Preise für ein Einzelzimmer liegen hier zwischen 129 und 149 DM .', 3.8215991233559306e-47), ('Leder : Vielleicht ringt Normann nur um Anerkennung .', 0.0)]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
