{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b782f73c-9519-4001-a494-46c79a83d4be",
   "metadata": {},
   "source": [
    "Download the homework data from Moodle. In the archive, you will find two files: Two German tokenized\n",
    "text with 50K lines each. Each line consists of a sentence; special tokens have been added at the beginning\n",
    "and at the end.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1831ab3-4dbd-441f-a89d-93407b0cacc1",
   "metadata": {},
   "source": [
    "Example: %^% %^% Leder : Vielleicht ringt Normann nur um Anerkennung . %$% %$%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4b5e3f-847f-4b72-b038-3974cb522094",
   "metadata": {},
   "source": [
    "This sentence has 9 tokens, 10 bigrams and 11 trigrams: note that the special tokens %^% and %$% are\n",
    "only considered if needed. Tokens are separated by a space character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "06b2eeb9-3d55-4e7d-9709-4c1b2c446e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "Erst um 0.30 Uhr konnte die Berufsfeuerwehr die Menschen auf offener Strecke mit Leitern aus dem havarierten Zug evakuieren .\n",
      "Zusammen sitzen sie am Tisch unter der Lampe im Wohnzimmer .\n"
     ]
    }
   ],
   "source": [
    "# read the files\n",
    "import os\n",
    "from random import randint\n",
    "prep = True\n",
    "data_dir = './de_text'\n",
    "train_file = 'de_text.test'\n",
    "test_file = 'de_text.train'\n",
    "\n",
    "def preprocess(to_process, start_pattern, end_pattern):\n",
    "    t = to_process\n",
    "    if start_pattern:\n",
    "        t = t.strip(start_pattern)\n",
    "    if end_pattern:\n",
    "        t = t.strip(end_pattern)\n",
    "    return t\n",
    "\n",
    "def read_and_prep(path_to_file, prep = True):\n",
    "    with open(path_to_file, encoding = 'utf-8') as f:\n",
    "        corp = [x.strip('\\n') for x in f]\n",
    "    if prep:\n",
    "        corp = [preprocess(x,'%^% %^%', '%$% %$%') for x in corp]\n",
    "    return corp\n",
    "\n",
    "train = read_and_prep(os.path.join(data_dir, train_file))\n",
    "print(len(train))\n",
    "print(train[randint(0, len(train))])\n",
    "print(train[randint(0, len(train))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8c80a495-7648-4e0e-a8da-e5c9bdda686b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a) List the 20 most frequent words from the training set.\n",
      "Top 20 most frequent token are [('.', 51601), (',', 43454), ('der', 24425), ('die', 23464), ('und', 15684), ('in', 13147), ('\"', 12934), ('den', 9621), ('von', 7450), ('zu', 7014), ('das', 6669), ('mit', 6434), ('sich', 6050), ('ist', 5955), ('auf', 5726), ('f√ºr', 5572), ('nicht', 5542), ('im', 5526), ('Die', 5429), ('des', 5186)]\n"
     ]
    }
   ],
   "source": [
    "print('a) List the 20 most frequent words from the training set.')\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "def count_token(corpus):\n",
    "    vocab = {}\n",
    "    for doc in corpus:\n",
    "        for token in doc.split(' '):\n",
    "            try:\n",
    "                vocab[token] += 1\n",
    "            except KeyError:\n",
    "                vocab[token] = 1\n",
    "    return vocab\n",
    "\n",
    "def get_n_most_freq(vocabulary, n=10):\n",
    "    return sorted(vocabulary.items(), key = itemgetter(1), reverse=True)[:n]\n",
    "    \n",
    "train_vocab = count_token(train)\n",
    "n = 20\n",
    "most_f = get_n_most_freq(train_vocab, n)\n",
    "print(f\"Top {n} most frequent token are {str(most_f)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6871be-735d-4f83-b299-f06f326d6738",
   "metadata": {},
   "source": [
    "### b) Compute the percentage of tokens in the test data that have not been seen in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0aa21174-e53d-436b-9553-429714a8cb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 107126 unique token in the test corpus. 61854 of them are unique or 57.74%\n"
     ]
    }
   ],
   "source": [
    "test = read_and_prep(os.path.join(data_dir, test_file))\n",
    "test_vocab = count_token(test)\n",
    "\n",
    "def get_unique_keys(dict1, dict2):\n",
    "    unique = 0\n",
    "    for k in dict1.keys():\n",
    "        if k not in dict2:\n",
    "            unique += 1\n",
    "    return unique\n",
    "\n",
    "unique_keys_in_test = get_unique_keys(test_vocab, train_vocab)\n",
    "print(f\"There are {len(test_vocab)} unique token in the test corpus. {unique_keys_in_test} of them are unique or {round(100 * unique_keys_in_test/len(test_vocab), 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58094fd-a8ab-47c3-8a91-9eece4863234",
   "metadata": {},
   "source": [
    "### d) Compute the percentage of bigrams in the test data that have not been seen in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d54b3c-be70-4870-bb9c-aaec38ae34ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_grams(n, path_to_corp):\n",
    "    \n",
    "    ngrams = {}\n",
    "    for doc in corpus:\n",
    "        tokens = doc.split(\" \")\n",
    "        # pad it\n",
    "        for i in range(n):\n",
    "            tokens.insert(0, '<bos>')\n",
    "            tokens.append('<eos>')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bc9647-525d-4a0e-8960-3619feec55c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
